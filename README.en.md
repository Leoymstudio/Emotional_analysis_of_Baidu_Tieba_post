# Emotional_analysis_of_Baidu_Tieba_post

1. Introduction

   Sentiment detection and data analysis of Baidu postings.
   Crawling the posts of Baidu posting bar with web crawler and analyzing the word cloud map, frequency counts and so on. And use Bayesian classification and clustering for sentiment detection.
   In which the sentiment analysis part uses the [weibo_senti_100k](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/weibo_senti_100k/intro.ipynb) dataset .
   This project is a part of the big Python assignment of Nanjing Agricultural University, and the defense PPT is attached in the project.

   #### Software Architecture

   Software Architecture Description

   "爬虫.py", "爬虫2.py", "爬虫3.py" are the crawler code for crawling Baidu posting, and the result is saved as xlsx file. Note: There is no CAPTCHA bypass here.
   The rest of the python file generates relevant graphs based on the title.
   机器学习-贝叶斯-聚类分析.py needs to import the categories generated by the cluster analysis before analyzing them.


   #### installation tutorial

   1. The libraries used here are scattered, so you can download them according to the py file.
   2. According to the compiler's hints, there are many new libraries that don't fit and need to be corrected.

   #### instructions for use

   1. Modify the crawler file to crawl (the crawler is adapted and adjusted by many regular expressions, which is more accurate for data crawling)

   ```python
   for page in range(0, XXXXXX the number of pages to be crawled XXXXXXXX):: url = f"{page
       url = f "https://tieba.baidu.com/p/XXXXXXXX要爬的网址XXXXXXXX?pn={page}"
       ............
   # Write all the data to an Excel file
   all_data_df.to_excel('XXXXXX save file XXXXXX.xlsx', index=False)
   print('All data saved as XXXXX save file XXXXXXXX.xlsx')
   ```

   2. Perform image generation

   ```python
   # Read the data from the Excel file
   
   df = pd.read_excel('XXXXXX save file XXXXXX.xlsx')
   comments = df['comment'].tolist()
   ..........
   ```

   3. Bayesian cluster analysis

   ```python
   # Perform sentiment analysis on a given topic Fill the command line output of clustering.py into the following table
   tp1= []
   tp2= []
   tp3= []
   tp4= []
   tp5= []
   ```

   #### Problems

   1. excellent sentiment recognition requires crawling a very large amount of data.

   2. good sentiment recognition requires newer models (the web is very iterative and requires newer evaluation criteria, some words have changed very much over the years) e.g. the posting data used in this paper would be a good and new dataset if it were manually categorized.

   3. more detailed emotional analysis standards (the emotion of the word should have more standards to measure, but should not be too much, easy to upset the judgment).
      Such as in recent years, the very hot mbti is clearly summarized in four criteria.

   4. regulation on the word division (jieba library has excellent effect on Chinese word division, but there is too fine effect on the network word, also easy to appear invalid data).
      Can try to use clustering + jieba for a new network word segmentation definition, in addition, the training of neural networks and rewrite the jieba library will also be a good approach.
